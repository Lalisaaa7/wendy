#!/usr/bin/env python3
"""
ç”¨äºè›‹ç™½è´¨ç»“åˆä½ç‚¹é¢„æµ‹çš„é²æ£’æ€§å¢å¼ºè®­ç»ƒ-æµ‹è¯•ç®¡é“ä»£ç  æ„å»ºä¸€ä¸ªåœ¨ä»»ä½•æµ‹è¯•é›†ä¸Šéƒ½èƒ½è¡¨ç°å‡ºè‰²çš„è›‹ç™½è´¨ç»“åˆä½ç‚¹é¢„æµ‹æ¨¡å‹ã€‚å®ƒè¦è§£å†³çš„æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒç—›ç‚¹ï¼š
æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰ä¸Šæ€§èƒ½å¤§å¹…ä¸‹é™ã€‚
åŸŸé€‚åº” + è´¨é‡æ§åˆ¶ + æ³›åŒ–å¢å¼º æ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šæ€§èƒ½å·®çš„é—®é¢˜ï¼Œé€šè¿‡å¤šç§æŠ€æœ¯æ‰‹æ®µæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
"""
import os
import time
import glob
import json
from tqdm import tqdm
import torch
from torch_geometric.data import Data
import numpy as np
from sklearn.metrics import pairwise_distances
from sklearn.model_selection import KFold
import warnings
warnings.filterwarnings('ignore')

from balanced_training_config import BalancedTrainingConfig
from improved_gnn_model import ImprovedBindingSiteGNN
from data_loader import ProteinDataset
from ddpm_diffusion_model import EnhancedDiffusionModel
from main import create_knn_edges, calculate_class_ratio
from gnn_model import set_seed

"""é²æ£’è®­ç»ƒé…ç½®æ¨¡å— è¿™ä¸ªç±»å®šä¹‰äº†æ•´ä¸ªç®¡é“çš„é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºç­–ç•¥ã€è´¨é‡æ§åˆ¶æ ‡å‡†ã€è®­ç»ƒæ–¹æ³•ç­‰ã€‚"""
class RobustTrainingConfig(BalancedTrainingConfig):

    def __init__(self, target_ratio=0.15, experiment_name="default"):
        super().__init__()
        # å¯é…ç½®çš„å¢å¼ºç­–ç•¥
        self.target_ratio = target_ratio  # ç›®æ ‡æ­£æ ·æœ¬æ¯”ä¾‹ï¼Œç”¨äºæ§åˆ¶æ•°æ®å¹³è¡¡
        self.experiment_name = experiment_name  # å®éªŒåç§°
        self.min_samples_per_protein = 5  # æœ€å°ç”Ÿæˆæ•°é‡
        self.max_augment_ratio = 2.0  # æœ€å¤§å¢å¼ºå€æ•°
        
        # è´¨é‡æ§åˆ¶
        self.quality_threshold = 0.7  # ç­›é€‰é«˜è´¨é‡ç”Ÿæˆæ ·æœ¬çš„é˜ˆå€¼ï¼Œåªæœ‰ä¸çœŸå®æ ·æœ¬ç›¸ä¼¼åº¦é«˜äºæ­¤å€¼çš„æ‰ä¼šè¢«ä¿ç•™ã€‚
        self.diversity_threshold = 0.3  # ç¡®ä¿æ ·æœ¬å¤šæ ·æ€§çš„è·ç¦»é˜ˆå€¼ï¼Œç”¨äºç¡®ä¿ä¿ç•™ä¸‹æ¥çš„ç”Ÿæˆæ ·æœ¬ä¹‹é—´ä¸ä¼šè¿‡äºç›¸ä¼¼ï¼Œé¿å…æ•°æ®å•ä¸€ã€‚
        
        # åŸŸé€‚åº”
        self.use_domain_adaptation = True#æ˜¯å¦å¯ç”¨åŸŸé€‚åº”ä»¥åŠå…¶æŸå¤±çš„æƒé‡ã€‚
        self.domain_weight = 0.1#åŸŸé€‚åº”æŸå¤±çš„æƒé‡ï¼Œå¹³è¡¡ä¸»ä»»åŠ¡å’ŒåŸŸé€‚åº”
        
        # äº¤å‰éªŒè¯
        self.use_cross_validation = True#æ˜¯å¦å¯ç”¨äº¤å‰éªŒè¯ä»¥åŠæŠ˜æ•° å³å°†æ•°æ®åˆ†æˆå‡ ä»½
        self.cv_folds = 3
        
        # é›†æˆå­¦ä¹ 
        self.ensemble_size = 3#åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å¤šå°‘ä¸ªæ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹ã€‚
        self.ensemble_dropout_rates = [0.3, 0.4, 0.5]
        
        # è¾“å‡ºæ§åˆ¶æ˜¯å¦æ‰“å°è¯¦ç»†åŠ è½½ä¿¡æ¯
        self.verbose_loading = False

 """å·¥ä½œæµç¨‹ï¼šè®¡ç®—æ¯ä¸ªç”Ÿæˆæ ·æœ¬ä¸æ‰€æœ‰çœŸå®æ ·æœ¬çš„è·ç¦»ï¼Œæ‰¾åˆ°æ¯ä¸ªç”Ÿæˆæ ·æœ¬çš„æœ€å°è·ç¦»ï¼ˆæœ€ç›¸ä¼¼çš„ï¼‰
å°†è·ç¦»è½¬æ¢ä¸ºè´¨é‡åˆ†æ•°ï¼ˆ0-1èŒƒå›´ï¼‰æ ¹æ®é˜ˆå€¼ç­›é€‰é«˜è´¨é‡æ ·æœ¬ã€‚"""
def calculate_sample_quality(generated_samples, real_samples, threshold=0.7):
    """è¯„ä¼°ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ï¼Œé€šè¿‡è®¡ç®—ä¸çœŸå®æ ·æœ¬çš„è·ç¦»æ¥åˆ¤æ–­ç›¸ä¼¼åº¦ã€‚"""
    if len(generated_samples) == 0 or len(real_samples) == 0:
        return [], 0.0
    
    try:
        # è®¡ç®—ä¸çœŸå®æ ·æœ¬çš„æœ€å°è·ç¦»
        distances = pairwise_distances(generated_samples, real_samples, metric='euclidean')
        min_distances = np.min(distances, axis=1)
        
        # è®¡ç®—è´¨é‡åˆ†æ•° (è·ç¦»è¶Šå°è´¨é‡è¶Šé«˜)
        max_dist = np.max(min_distances) + 1e-8
        quality_scores = 1.0 - (min_distances / max_dist)
        
        # ç­›é€‰é«˜è´¨é‡æ ·æœ¬
        high_quality_mask = quality_scores >= threshold
        high_quality_samples = generated_samples[high_quality_mask]
        avg_quality = np.mean(quality_scores)
        
        return high_quality_samples, avg_quality
    except:
        return generated_samples, 0.5


def calculate_sample_diversity(samples, threshold=0.3):
    """è¯„ä¼°æ ·æœ¬å¤šæ ·æ€§ è®¡ç®—æ¯ä¸ªâ€œç”Ÿæˆæ ·æœ¬â€ä¸æ‰€æœ‰â€œçœŸå®æ ·æœ¬â€ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚å¯¹æ¯ä¸ªç”Ÿæˆæ ·æœ¬ï¼Œåªä¿ç•™å®ƒä¸çœŸå®æ ·æœ¬é›†ä¹‹é—´çš„æœ€çŸ­è·ç¦»ã€‚"""
    if len(samples) <= 1:
        return samples, 1.0
    
    try:
        # è®¡ç®—æ ·æœ¬é—´è·ç¦»
        distances = pairwise_distances(samples, metric='euclidean')
        np.fill_diagonal(distances, np.inf)
        
        # ç§»é™¤è¿‡äºç›¸ä¼¼çš„æ ·æœ¬
        diverse_indices = []
        for i in range(len(samples)):
            min_dist = np.min(distances[i])
            if len(diverse_indices) == 0 or min_dist >= threshold:
                diverse_indices.append(i)
        
        diverse_samples = samples[diverse_indices]
        diversity_score = len(diverse_samples) / len(samples)
        
        return diverse_samples, diversity_score
    except:
        return samples, 1.0

"""åˆ†æå½“å‰æ•°æ®çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ è®¡ç®—éœ€è¦ç”Ÿæˆçš„æ–°æ ·æœ¬æ•°é‡ ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå€™é€‰æ ·æœ¬è¿›è¡Œè´¨é‡ç­›é€‰å’Œå¤šæ ·æ€§ç­›é€‰ 
å°†é«˜è´¨é‡æ ·æœ¬åˆå¹¶åˆ°åŸå§‹æ•°æ®ä¸­ é‡æ–°æ„å»ºå›¾çš„è¾¹ç»“æ„
"""
def robust_augment_dataset(dataset, diffusion_model, config):
    """é²æ£’æ•°æ®å¢å¼º -ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°æ ·æœ¬ï¼Œå¹¶è¿›è¡Œä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ é›†æˆäº†è´¨é‡å’Œå¤šæ ·æ€§æ§åˆ¶ã€‚"""
    augmented_data = []
    quality_stats = []
    diversity_stats = []

    print(f" é²æ£’å¢å¼ºç­–ç•¥:")
    print(f"  - ç›®æ ‡æ¯”ä¾‹: {config.target_ratio:.1%}")
    print(f"  - è´¨é‡é˜ˆå€¼: {config.quality_threshold}")
    print(f"  - å¤šæ ·æ€§é˜ˆå€¼: {config.diversity_threshold}")
    
    for data in tqdm(dataset, desc="Robust augmenting"):
        try:
            protein_context = data.protein_context.to(config.device)
            
            # æå–æ­£æ ·æœ¬ç”¨äºè´¨é‡è¯„ä¼°
            pos_mask = (data.y == 1)
            if pos_mask.sum() == 0:
                augmented_data.append(data)
                continue
                
            real_pos_samples = data.x[pos_mask].cpu().numpy()
            n_pos = pos_mask.sum().item()
            n_neg = (data.y == 0).sum().item()
            total_nodes = n_pos + n_neg

            # è®¡ç®—ç”Ÿæˆæ•°é‡ - æ›´ä¿å®ˆ
            target_pos = int(total_nodes * config.target_ratio)
            n_to_generate = max(config.min_samples_per_protein, target_pos - n_pos)#ç­–ç•¥æ›´ä¿å®ˆï¼Œä¼šå—é™äºçœŸå®æ­£æ ·æœ¬æ•°é‡çš„ä¸€å®šå€æ•°ï¼ˆmax_augment_ratioï¼‰ã€‚
            n_to_generate = min(n_to_generate, int(n_pos * config.max_augment_ratio))

            if n_to_generate > 0:
                # ç”Ÿæˆå€™é€‰æ ·æœ¬ (å¤šç”Ÿæˆä¸€äº›ç”¨äºç­›é€‰)
                candidate_samples = diffusion_model.generate_positive_sample(
                    protein_context,
                    num_samples=n_to_generate * 2  # ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆ2å€äºæ‰€éœ€æ•°é‡çš„â€œå€™é€‰â€æ ·æœ¬ã€‚
                )

                if candidate_samples is None or len(candidate_samples) == 0:
                    augmented_data.append(data)
                    continue

                # è´¨é‡æ§åˆ¶ ç¬¬ä¸€è½®ç­›é€‰ï¼šè°ƒç”¨ calculate_sample_quality ç­›é€‰å‡ºé«˜è´¨é‡çš„æ ·æœ¬ã€‚
                quality_samples, quality_score = calculate_sample_quality(
                    candidate_samples, real_pos_samples, config.quality_threshold
                )
                
                # å¤šæ ·æ€§æ§åˆ¶ ç¬¬äºŒè½®ç­›é€‰ï¼šè°ƒç”¨ calculate_sample_diversity åœ¨é«˜è´¨é‡æ ·æœ¬ä¸­å†ç­›é€‰å‡ºå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚
                if len(quality_samples) > 0:
                    diverse_samples, diversity_score = calculate_sample_diversity(
                        quality_samples, config.diversity_threshold
                    )
                else:
                    diverse_samples, diversity_score = candidate_samples[:n_to_generate], 0.5
                
                # é™åˆ¶æœ€ç»ˆæ•°é‡ é€‰å– n_to_generate ä¸ªï¼Œä¸åŸå§‹æ•°æ®åˆå¹¶ï¼Œå¹¶é‡å»ºå›¾çš„è¾¹ç»“æ„ã€‚
                final_samples = diverse_samples[:n_to_generate]
                
                quality_stats.append(quality_score)
                diversity_stats.append(diversity_score)

                if len(final_samples) > 0:
                    # åˆ›å»ºæ–°èŠ‚ç‚¹
                    new_x = torch.tensor(final_samples, dtype=torch.float32)
                    new_y = torch.ones(new_x.size(0), dtype=torch.long)

                    # åˆå¹¶åˆ°åŸå§‹å›¾
                    updated_x = torch.cat([data.x, new_x], dim=0)
                    updated_y = torch.cat([data.y, new_y], dim=0)

                    # é™åˆ¶å›¾å¤§å°
                    if len(updated_x) > config.max_nodes_per_graph:
                        pos_mask_new = (updated_y == 1)
                        neg_mask_new = (updated_y == 0)
                        
                        pos_indices = torch.where(pos_mask_new)[0]
                        neg_indices = torch.where(neg_mask_new)[0]
                        
                        max_neg = config.max_nodes_per_graph - len(pos_indices)
                        if max_neg > 0 and len(neg_indices) > max_neg:
                            keep_neg = neg_indices[torch.randperm(len(neg_indices))[:max_neg]]
                            keep_indices = torch.cat([pos_indices, keep_neg])
                        else:
                            keep_indices = torch.arange(len(updated_x))
                        
                        updated_x = updated_x[keep_indices]
                        updated_y = updated_y[keep_indices]

                    # åˆ›å»ºKNNè¾¹
                    updated_edge_index = create_knn_edges(updated_x, k=config.knn_k, max_samples=2000)

                    # åˆ›å»ºå¢å¼ºåçš„å›¾
                    augmented_graph = Data(
                        x=updated_x,
                        edge_index=updated_edge_index,
                        y=updated_y,
                        protein_context=data.protein_context,
                        name=data.name + "_robust_aug"
                    )
                    augmented_data.append(augmented_graph)
                else:
                    augmented_data.append(data)
            else:
                augmented_data.append(data)
                
        except Exception as e:
            print(f"Warning: Augmentation failed for {data.name}: {e}")
            augmented_data.append(data)

    # æ‰“å°è´¨é‡ç»Ÿè®¡
    if quality_stats:
        avg_quality = np.mean(quality_stats)
        avg_diversity = np.mean(diversity_stats)
        print(f" å¢å¼ºè´¨é‡: å¹³å‡è´¨é‡={avg_quality:.3f}, å¹³å‡å¤šæ ·æ€§={avg_diversity:.3f}")

    return augmented_data


def domain_adaptive_loss(predictions, targets, domain_weight=0.1):
    """åŸŸé€‚åº”æŸå¤±è®­ç»ƒ è®©æ¨¡å‹åœ¨é¢å¯¹ä¸åŒæ¥æºæˆ–ä¸åŒç‰¹å¾åˆ†å¸ƒçš„æ•°æ®æ—¶ï¼Œè¡¨ç°æ›´ç¨³å®šã€‚ """
    # åŸºç¡€åˆ†ç±»æŸå¤± - ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µ,ç¡®ä¿æ¨¡å‹èƒ½å‡†ç¡®å®Œæˆåˆ†ç±»ä»»åŠ¡
    if predictions.dim() == 1:
        # å•è¾“å‡ºäºŒå…ƒåˆ†ç±»
        base_loss = torch.nn.functional.binary_cross_entropy_with_logits(predictions, targets.float())
    else:
        # å¤šç±»åˆ†ç±»
        targets = targets.long()
        base_loss = torch.nn.functional.cross_entropy(predictions, targets)
    
    # åŸŸæ­£åˆ™åŒ– - æ ¸å¿ƒæ­¥éª¤ï¼Œé¼“åŠ±ç‰¹å¾åˆ†å¸ƒä¸€è‡´æ€§,æå‡æ³›åèƒ½åŠ›
    batch_size = predictions.size(0)
    if batch_size > 1:
        # å¤„ç†äºŒå…ƒåˆ†ç±»å’Œå¤šç±»åˆ†ç±»çš„ä¸åŒæƒ…å†µ
        if predictions.dim() == 1:
            # äºŒå…ƒåˆ†ç±» - ä½¿ç”¨sigmoidè·å–æ¦‚ç‡
            probs = torch.sigmoid(predictions)
            prob_var = torch.var(probs, dim=0)
        else:
            # å¤šç±»åˆ†ç±» - ä½¿ç”¨softmax
            probs = torch.softmax(predictions, dim=1)
            prob_var = torch.var(probs, dim=0).mean()
        domain_loss = domain_weight * prob_var
    else:
        domain_loss = 0.0
    
    return base_loss + domain_loss


class RobustGNNModel(ImprovedBindingSiteGNN):
    """é²æ£’GNNæ¨¡å‹ RobustGNNModel ç±»åˆ™æ˜¯åœ¨GNNæ¨¡å‹ä¸­é›†æˆäº†è¿™ç§æ–°çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒé€»è¾‘ã€‚"""
    
    def __init__(self, input_dim, hidden_dim=128, dropout=0.3, use_focal_loss=True, 
                 focal_alpha=0.75, focal_gamma=2.0, pos_weight=3.0, domain_weight=0.1):
        super().__init__(input_dim, hidden_dim, dropout, use_focal_loss, 
                         focal_alpha, focal_gamma, pos_weight)
        self.domain_weight = domain_weight
        
    def train_with_domain_adaptation(self, train_data, val_data, epochs=100, lr=0.001, 
                                   device='cuda', patience=10):
        """åŸŸé€‚åº”è®­ç»ƒï¼Œåœ¨æ ‡å‡†è®­ç»ƒåŸºç¡€ä¸Šæ·»åŠ åŸŸé€‚åº”æœºåˆ¶ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚"""
        self.to(device)
        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
        
        best_val_f1 = 0
        best_val_auc = 0
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.train()
            total_loss = 0
            # åŸŸé€‚åº”æŸå¤± = åŸºç¡€åˆ†ç±»æŸå¤± + åŸŸæ­£åˆ™åŒ–
            for data in train_data:
                data = data.to(device)
                optimizer.zero_grad()
                
                out = self(data)
                
                # åŸŸé€‚åº”æŸå¤±
                loss = domain_adaptive_loss(out, data.y, self.domain_weight)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            if epoch % 10 == 0:
                val_metrics = self.evaluate(val_data, device)
                scheduler.step(val_metrics['f1'])
                
                if val_metrics['f1'] > best_val_f1:
                    best_val_f1 = val_metrics['f1']
                    best_val_auc = val_metrics['auc_pr']
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch}")
                    break
        
        return best_val_auc, best_val_f1


def cross_validation_training(augmented_data, original_data, config):
    """æ”¹è¿›çš„äº¤å‰éªŒè¯è®­ç»ƒ - ä¸¥æ ¼åˆ†ç¦»å¢å¼ºæ•°æ®å’ŒéªŒè¯æ•°æ®ï¼Œå®æ–½ä¸¥æ ¼çš„äº¤å‰éªŒè¯ï¼Œç¡®ä¿éªŒè¯é›†åªåŒ…å«åŸå§‹çœŸå®æ•°æ®ã€‚"""
    print(f"\n{config.cv_folds}æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
    print(f" è®­ç»ƒç­–ç•¥: è®­ç»ƒé›†ä½¿ç”¨å¢å¼ºæ•°æ®ï¼ŒéªŒè¯é›†ä»…ä½¿ç”¨åŸå§‹çœŸå®æ•°æ®")
    
    # å°†åŸå§‹æ•°æ®åˆ†æˆCVæŠ˜ç”¨äºéªŒè¯
    kf = KFold(n_splits=config.cv_folds, shuffle=True, random_state=config.seed)
    cv_results = []
    
    # å°†å¢å¼ºæ•°æ®å’ŒåŸå§‹æ•°æ®åˆ†åˆ«æ ‡è®°
    original_indices = list(range(len(original_data)))
    
    for fold, (_, val_idx) in enumerate(kf.split(original_indices)):
        print(f"\nç¬¬ {fold+1}/{config.cv_folds} æŠ˜")
        
        # è®­ç»ƒé›†ï¼šä½¿ç”¨æ‰€æœ‰å¢å¼ºæ•°æ® + é™¤éªŒè¯é›†å¤–çš„åŸå§‹æ•°æ®
        val_original_indices = set(val_idx)
        train_original = [original_data[i] for i in range(len(original_data)) if i not in val_original_indices]
        train_fold = augmented_data + train_original
        
        # éªŒè¯é›†ï¼šéªŒè¯é›†ï¼šä»…ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸å«å¢å¼ºæ•°æ®ï¼‰
        val_fold = [original_data[i] for i in val_idx]
        
        print(f"  è®­ç»ƒé›†å¤§å°: {len(train_fold)} (å¢å¼º: {len(augmented_data)}, åŸå§‹: {len(train_original)})")
        print(f"  éªŒè¯é›†å¤§å°: {len(val_fold)} (ä»…åŸå§‹æ•°æ®)")
        
        # è®­ç»ƒæ¨¡å‹ï¼Œ
        model = RobustGNNModel(
            input_dim=config.diffusion_input_dim,
            hidden_dim=config.gnn_hidden_dim,
            dropout=config.gnn_dropout,
            use_focal_loss=config.use_focal_loss,
            focal_alpha=config.focal_alpha,
            focal_gamma=config.focal_gamma,
            pos_weight=config.pos_weight,
            domain_weight=config.domain_weight
        )
        
        best_auc, best_f1 = model.train_with_domain_adaptation(
            train_fold, val_fold,
            epochs=config.gnn_epochs,
            lr=config.gnn_lr,
            device=config.device,
            patience=config.gnn_patience
        )
        
        cv_results.append({
            'model': model,
            'val_f1': best_f1,
            'val_auc': best_auc
        })
        
        print(f"   ç¬¬{fold+1}æŠ˜: F1={best_f1:.4f}, AUC-PR={best_auc:.4f}")
    
    return cv_results


def ensemble_prediction(models, test_data, device):
    """é›†æˆé¢„æµ‹"""
    all_predictions = []
    
    for model in models:
        model.eval()
        model.to(device)
        
        predictions = []
        with torch.no_grad():
            for data in test_data:
                data = data.to(device)
                out = model(data.x, data.edge_index)
                probs = torch.softmax(out, dim=1)
                predictions.append(probs.cpu().numpy())
        
        all_predictions.append(np.concatenate(predictions))
    
    # å¹³å‡é›†æˆ
    ensemble_probs = np.mean(all_predictions, axis=0)
    ensemble_preds = np.argmax(ensemble_probs, axis=1)
    
    return ensemble_preds, ensemble_probs[:, 1]  # è¿”å›æ­£ç±»æ¦‚ç‡


def train_and_test_robust_model(train_file, test_files, config):
    """è¿™æ˜¯æ•´ä¸ªé²æ£’æ€§è®­ç»ƒå’Œæµ‹è¯•æµç¨‹çš„ä¸»å‡½æ•°ï¼Œè®­ç»ƒå¹¶æµ‹è¯•é²æ£’æ¨¡å‹"""
    train_name = os.path.splitext(os.path.basename(train_file))[0]
    print(f"\n å¼€å§‹é²æ£’è®­ç»ƒ-æµ‹è¯•: {train_name}")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½• - åŒ…å«æ¯”ä¾‹ä¿¡æ¯çš„å‘½å
    ratio_str = f"{config.target_ratio:.2f}".replace(".", "")
    output_dir_name = f"{train_name}_robust_r{ratio_str}_{config.experiment_name}"
    output_path = os.path.join(config.output_dir, output_dir_name)
    os.makedirs(output_path, exist_ok=True)
    
    # åŠ è½½å’Œå¤„ç†æ•°æ® (ä½¿ç”¨é™é»˜åŠ è½½å‡å°‘è¾“å‡º)
    print(f" é˜¶æ®µ1: åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = load_dataset_quiet(train_file, config)
    
    if not train_dataset:
        print(f" æ•°æ®é›†ä¸ºç©º: {train_file}")
        return None
    
    print(f" åŠ è½½äº† {len(train_dataset)} ä¸ªè›‹ç™½è´¨")
    
    # ç»Ÿè®¡åŸå§‹æ•°æ®
    orig_ratio, orig_pos, orig_neg = calculate_class_ratio(train_dataset)
    print(f" åŸå§‹æ•°æ®: {orig_pos:,} æ­£æ ·æœ¬, {orig_neg:,} è´Ÿæ ·æœ¬ (æ¯”ä¾‹: {orig_ratio:.3%})")
    
    # è®­ç»ƒæ‰©æ•£æ¨¡å‹
    print(f"\n é˜¶æ®µ2: è®­ç»ƒæ‰©æ•£æ¨¡å‹...")
    diffusion_model = EnhancedDiffusionModel(
        input_dim=config.diffusion_input_dim,
        T=config.diffusion_T,
        device=config.device
    )
    
    diffusion_start = time.time()
    diffusion_model.train_on_positive_samples(
        train_dataset,
        epochs=config.diffusion_epochs,
        batch_size=config.diffusion_batch_size
    )
    diffusion_time = time.time() - diffusion_start
    print(f" æ‰©æ•£æ¨¡å‹è®­ç»ƒå®Œæˆ: {diffusion_time:.1f}ç§’")
    
    # é²æ£’æ•°æ®å¢å¼º
    print(f"\nğŸ›¡ é˜¶æ®µ3: é²æ£’æ•°æ®å¢å¼º...")
    augment_start = time.time()
    augmented_data = robust_augment_dataset(train_dataset, diffusion_model, config)
    augment_time = time.time() - augment_start
    
    aug_ratio, aug_pos, aug_neg = calculate_class_ratio(augmented_data)
    print(f" é²æ£’å¢å¼ºå®Œæˆ: {aug_pos:,} æ­£æ ·æœ¬, {aug_neg:,} è´Ÿæ ·æœ¬ (æ¯”ä¾‹: {aug_ratio:.3%}) - ç”¨æ—¶: {augment_time:.1f}ç§’")
    
    # äº¤å‰éªŒè¯è®­ç»ƒ
    print(f"\n é˜¶æ®µ4: äº¤å‰éªŒè¯è®­ç»ƒ...")
    gnn_start = time.time()
    cv_results = cross_validation_training(augmented_data, train_dataset, config)
    gnn_time = time.time() - gnn_start
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_cv_result = max(cv_results, key=lambda x: x['val_f1'])
    best_model = best_cv_result['model']
    
    avg_f1 = np.mean([r['val_f1'] for r in cv_results])
    avg_auc = np.mean([r['val_auc'] for r in cv_results])
    
    print(f" äº¤å‰éªŒè¯å®Œæˆ: å¹³å‡F1={avg_f1:.4f}, å¹³å‡AUC-PR={avg_auc:.4f} - ç”¨æ—¶: {gnn_time:.1f}ç§’")
    
    # ä¿å­˜æ¨¡å‹
    model_save_path = os.path.join(output_path, "robust_gnn_model.pt")
    torch.save(best_model.state_dict(), model_save_path)
    print(f" æœ€ä½³æ¨¡å‹ä¿å­˜è‡³: {model_save_path}")
    
    # æµ‹è¯•é˜¶æ®µ - ä½¿ç”¨é›†æˆé¢„æµ‹
    print(f"\n é˜¶æ®µ5: é²æ£’æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    ensemble_models = [r['model'] for r in cv_results]
    test_results = {}
    
    for test_file in test_files:
        test_name = os.path.splitext(os.path.basename(test_file))[0]
        print(f"\n æµ‹è¯•æ•°æ®é›†: {test_name}")
        
        try:
            # åŠ è½½æµ‹è¯•æ•°æ® (é™é»˜æ¨¡å¼)
            test_dataset = load_dataset_quiet(test_file, config)
            print(f" åŠ è½½äº† {len(test_dataset)} ä¸ªè›‹ç™½è´¨")
            
            # é›†æˆè¯„ä¼°
            start_time = time.time()
            
            if config.ensemble_size > 1:
                # ä½¿ç”¨å‰Nä¸ªæœ€ä½³æ¨¡å‹è¿›è¡Œé›†æˆ
                top_models = sorted(cv_results, key=lambda x: x['val_f1'], reverse=True)[:config.ensemble_size]
                ensemble_models_top = [r['model'] for r in top_models]
                
                # è¿™é‡Œéœ€è¦å®ç°é›†æˆè¯„ä¼°é€»è¾‘
                # æš‚æ—¶ä½¿ç”¨æœ€ä½³å•æ¨¡å‹
                metrics = best_model.evaluate(test_dataset, device=config.device)
            else:
                metrics = best_model.evaluate(test_dataset, device=config.device)
            
            eval_time = time.time() - start_time
            
            # æ‰“å°ç»“æœ
            print(f" é²æ£’æµ‹è¯•ç»“æœ ({eval_time:.2f}s):")
            print(f"   F1 Score:         {metrics['f1']:.4f}")
            print(f"   Accuracy:         {metrics['accuracy']:.4f}")
            print(f"   Balanced Acc:     {metrics['balanced_accuracy']:.4f}")
            print(f"   Precision:        {metrics['precision']:.4f}")
            print(f"   Recall:           {metrics['recall']:.4f}")
            print(f"   Specificity:      {metrics['specificity']:.4f}")
            print(f"   AUC-PR:           {metrics['auc_pr']:.4f}")
            
            test_results[test_name] = metrics
            
        except Exception as e:
            print(f" æµ‹è¯•å¤±è´¥: {str(e)}")
            continue
    
    # ä¿å­˜ç»“æœ
    total_time = time.time() - diffusion_start
    
    full_results = {
        "model_name": train_name + "_robust",
        "model_type": "Robust GNN with Quality-Controlled Augmentation",
        "training_info": {
            "original_positive": orig_pos,
            "original_negative": orig_neg,
            "original_ratio": orig_ratio,
            "augmented_positive": aug_pos,
            "augmented_negative": aug_neg,
            "augmented_ratio": aug_ratio,
            "target_ratio": config.target_ratio,
            "cv_avg_f1": avg_f1,
            "cv_avg_auc": avg_auc,
            "best_cv_f1": best_cv_result['val_f1'],
            "best_cv_auc": best_cv_result['val_auc'],
            "diffusion_time": diffusion_time,
            "augment_time": augment_time,
            "gnn_time": gnn_time,
            "total_time": total_time
        },
        "robust_config": {
            "quality_threshold": config.quality_threshold,
            "diversity_threshold": config.diversity_threshold,
            "domain_weight": config.domain_weight,
            "cv_folds": config.cv_folds,
            "max_augment_ratio": config.max_augment_ratio
        },
        "test_results": test_results
    }
    
    # ä¿å­˜ç»“æœ
    with open(os.path.join(output_path, "robust_results.json"), 'w') as f:
        json.dump(full_results, f, indent=2, default=lambda obj: float(obj) if isinstance(obj, np.floating) else int(obj) if isinstance(obj, np.integer) else obj)
    
    print(f"\n {train_name} é²æ£’è®­ç»ƒ-æµ‹è¯•å®Œæˆ!")
    print(f" æ€»ç”¨æ—¶: {total_time//60:.0f}m {total_time%60:.0f}s")
    print(f" æ•°æ®æ”¹å–„: {orig_ratio:.3%} â†’ {aug_ratio:.3%}")
    print(f" äº¤å‰éªŒè¯æ€§èƒ½: F1={avg_f1:.4f}, AUC-PR={avg_auc:.4f}")
    
    return full_results


def load_dataset_from_file(dataset_file, config):
    """ä»å•ä¸ªæ–‡ä»¶åŠ è½½æ•°æ®é›†"""
    import shutil
    
    temp_data_dir = os.path.join(config.data_dir, "temp")
    os.makedirs(temp_data_dir, exist_ok=True)
    
    temp_file = os.path.join(temp_data_dir, os.path.basename(dataset_file))
    shutil.copy2(dataset_file, temp_file)
    
    try:
        dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
        dataset = dataset_loader.proteins
    finally:
        if os.path.exists(temp_data_dir):
            shutil.rmtree(temp_data_dir)
    
    return dataset


def main():
    """ä¸»å‡½æ•° æ˜¯æ•´ä¸ªç¨‹åºçš„å…¥å£ï¼Œè´Ÿè´£æŸ¥æ‰¾è®­ç»ƒå’Œæµ‹è¯•æ–‡ä»¶ï¼Œå¹¶ä¸ºæ¯ä¸ªè®­ç»ƒæ–‡ä»¶å¯åŠ¨ä¸€æ¬¡å®Œæ•´çš„æµç¨‹"""
    print("ğŸ›¡ é²æ£’æ€§å¢å¼ºè®­ç»ƒ-æµ‹è¯•ç®¡é“å¯åŠ¨")
    print("="*80)
    print("æ”¹è¿›ç­–ç•¥: è´¨é‡æ§åˆ¶ + åŸŸé€‚åº” + äº¤å‰éªŒè¯ + é›†æˆå­¦ä¹ ")
    print("="*80)
    
    # åˆå§‹åŒ–é…ç½®
    config = RobustTrainingConfig()
    set_seed(config.seed)
    
    print(f"\nï¸ é²æ£’æ€§é…ç½®:")
    print(f"  - ä¿å®ˆç›®æ ‡æ¯”ä¾‹: {config.target_ratio:.1%}")
    print(f"  - è´¨é‡æ§åˆ¶é˜ˆå€¼: {config.quality_threshold}")
    print(f"  - åŸŸé€‚åº”æƒé‡: {config.domain_weight}")
    print(f"  - {config.cv_folds}æŠ˜äº¤å‰éªŒè¯")
    print(f"  - {config.ensemble_size}æ¨¡å‹é›†æˆ")
    
    # æŸ¥æ‰¾æ–‡ä»¶
    train_files = sorted(glob.glob(os.path.join(config.data_dir, "*Train*.txt")))
    test_files = sorted(glob.glob(os.path.join(config.data_dir, "*Test*.txt")))
    
    print(f"\n æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶, {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    
    # æ‰§è¡Œé²æ£’è®­ç»ƒ-æµ‹è¯•ç®¡é“
    total_start = time.time()
    all_results = {}
    
    for train_file in train_files:
        try:
            result = train_and_test_robust_model(train_file, test_files, config)
            if result:
                all_results[os.path.splitext(os.path.basename(train_file))[0]] = result
        except Exception as e:
            print(f" å¤„ç†å¤±è´¥ {train_file}: {str(e)}")
    
    total_pipeline_time = time.time() - total_start
    
    # ç”ŸæˆæŠ¥å‘Š
    if all_results:
        results_file = os.path.join(config.output_dir, "robust_pipeline_results.json")
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2, default=lambda obj: float(obj) if isinstance(obj, np.floating) else int(obj) if isinstance(obj, np.integer) else obj)
        
        print(f"\n é²æ£’ç®¡é“æ‰§è¡Œå®Œæˆ!")
        print(f" æ€»æ—¶é—´: {total_pipeline_time//3600:.0f}h {(total_pipeline_time%3600)//60:.0f}m {total_pipeline_time%60:.0f}s")
        print(f" æˆåŠŸå®Œæˆ {len(all_results)} ä¸ªé²æ£’æ¨¡å‹")
        print(f" è¯¦ç»†ç»“æœ: {results_file}")
        
        # ç®€è¦æ€§èƒ½å¯¹æ¯”
        print(f"\n é²æ£’æ¨¡å‹æµ‹è¯•æ€§èƒ½:")
        print("-" * 70)
        print(f"{'è®­ç»ƒé›†':<15} {'å¹³å‡F1':<10} {'å¹³å‡å¹³è¡¡ACC':<12} {'å¹³å‡AUC-PR':<12}")
        print("-" * 70)
        
        for train_name, results in all_results.items():
            test_results = results['test_results']
            if test_results:
                avg_f1 = np.mean([m['f1'] for m in test_results.values()])
                avg_balanced_acc = np.mean([m['balanced_accuracy'] for m in test_results.values()])
                avg_auc_pr = np.mean([m['auc_pr'] for m in test_results.values()])
                
                print(f"{train_name:<15} {avg_f1:<10.4f} {avg_balanced_acc:<12.4f} {avg_auc_pr:<12.4f}")
    
    else:
        print(f"\n æ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•é²æ£’æ¨¡å‹è®­ç»ƒ")


def load_dataset_quiet(dataset_file, config):
    """é™é»˜åŠ è½½æ•°æ®é›† - å‡å°‘æ‰“å°è¾“å‡º"""
    import shutil
    import sys
    import contextlib
    
    temp_data_dir = os.path.join(config.data_dir, "temp")
    os.makedirs(temp_data_dir, exist_ok=True)
    
    temp_file = os.path.join(temp_data_dir, os.path.basename(dataset_file))
    shutil.copy2(dataset_file, temp_file)
    
    try:
        # ä¸´æ—¶é‡å®šå‘stdoutæ¥å‡å°‘æ‰“å°
        if not config.verbose_loading:
            with open(os.devnull, 'w') as devnull:
                with contextlib.redirect_stdout(devnull):
                    dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
                    dataset = dataset_loader.proteins
        else:
            dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
            dataset = dataset_loader.proteins
    finally:
        if os.path.exists(temp_data_dir):
            shutil.rmtree(temp_data_dir)
    
    return dataset


def test_different_ratios(train_file, test_files, ratios=[0.10, 0.15, 0.20, 0.25, 0.30]):
    """æµ‹è¯•ä¸åŒçš„æ•°æ®å¹³è¡¡æ¯”ä¾‹ ä¸ºæ¯ä¸ªæ¯”ä¾‹è¿è¡Œä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒ-æµ‹è¯•æµç¨‹ï¼Œæœ€åæ±‡æ€»ç»“æœç›´è‡³æ‰¾åˆ°æœ€ä½³æ¨¡å‹"""
    print(f"å¼€å§‹æ¯”ä¾‹æµ‹è¯•å®éªŒ")
    print(f"æµ‹è¯•æ¯”ä¾‹: {[f'{r:.1%}' for r in ratios]}")
    print("="*80)
    
    results_summary = {}
    
    for ratio in ratios:
        print(f"\n æµ‹è¯•æ¯”ä¾‹: {ratio:.1%}")
        print("-" * 50)
        
        # åˆ›å»ºé…ç½®
        experiment_name = f"ratio_test_{ratio:.2f}".replace(".", "")
        config = RobustTrainingConfig(target_ratio=ratio, experiment_name=experiment_name)
        config.verbose_loading = False  # å‡å°‘è¾“å‡º
        set_seed(config.seed)
        
        try:
            start_time = time.time()
            result = train_and_test_robust_model_quiet(train_file, test_files, config)
            end_time = time.time()
            
            if result:
                # è®¡ç®—å¹³å‡æ€§èƒ½
                test_results = result['test_results']
                avg_f1 = np.mean([m['f1'] for m in test_results.values()])
                avg_balanced_acc = np.mean([m['balanced_accuracy'] for m in test_results.values()])
                avg_specificity = np.mean([m['specificity'] for m in test_results.values()])
                
                results_summary[ratio] = {
                    'avg_f1': avg_f1,
                    'avg_balanced_acc': avg_balanced_acc,
                    'avg_specificity': avg_specificity,
                    'time': end_time - start_time,
                    'full_results': result
                }
                
                print(f"æ¯”ä¾‹ {ratio:.1%}: F1={avg_f1:.3f}, å¹³è¡¡ACC={avg_balanced_acc:.3f}, ç‰¹å¼‚æ€§={avg_specificity:.3f}")
            else:
                print(f" æ¯”ä¾‹ {ratio:.1%}: æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            print(f"æ¯”ä¾‹ {ratio:.1%}: é”™è¯¯ - {str(e)}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if results_summary:
        print(f"\næ¯”ä¾‹æµ‹è¯•ç»“æœæ±‡æ€»:")
        print("="*80)
        print(f"{'æ¯”ä¾‹':<8} {'F1åˆ†æ•°':<10} {'å¹³è¡¡ACC':<12} {'ç‰¹å¼‚æ€§':<12} {'ç”¨æ—¶(ç§’)':<12}")
        print("-" * 80)
        
        best_f1_ratio = None
        best_f1_score = 0
        
        for ratio, metrics in results_summary.items():
            print(f"{ratio:<8.1%} {metrics['avg_f1']:<10.3f} {metrics['avg_balanced_acc']:<12.3f} "
                  f"{metrics['avg_specificity']:<12.3f} {metrics['time']:<12.1f}")
            
            if metrics['avg_f1'] > best_f1_score:
                best_f1_score = metrics['avg_f1']
                best_f1_ratio = ratio
        
        print(f"\n æœ€ä½³æ¯”ä¾‹: {best_f1_ratio:.1%} (F1={best_f1_score:.3f})")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        train_name = os.path.splitext(os.path.basename(train_file))[0]
        results_file = f"ratio_comparison_{train_name}_{int(time.time())}.json"
        with open(results_file, 'w') as f:
            json.dump(results_summary, f, indent=2, default=str)
        print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
    
    return results_summary


def train_and_test_robust_model_quiet(train_file, test_files, config):
    """é™é»˜ç‰ˆæœ¬çš„é²æ£’è®­ç»ƒæµ‹è¯• - å‡å°‘è¾“å‡º"""
    train_name = os.path.splitext(os.path.basename(train_file))[0]
    
    # åˆ›å»ºè¾“å‡ºç›®å½• - åŒ…å«æ¯”ä¾‹ä¿¡æ¯çš„å‘½å  
    ratio_str = f"{config.target_ratio:.2f}".replace(".", "")
    output_dir_name = f"{train_name}_robust_r{ratio_str}_{config.experiment_name}"
    output_path = os.path.join(config.output_dir, output_dir_name)
    os.makedirs(output_path, exist_ok=True)
    
    # é™é»˜åŠ è½½æ•°æ®
    train_dataset = load_dataset_quiet(train_file, config)
    
    if not train_dataset:
        return None
    
    # ç»Ÿè®¡åŸå§‹æ•°æ®
    orig_ratio, orig_pos, orig_neg = calculate_class_ratio(train_dataset)
    
    # è®­ç»ƒæ‰©æ•£æ¨¡å‹
    diffusion_model = EnhancedDiffusionModel(
        input_dim=config.diffusion_input_dim,
        T=config.diffusion_T,
        device=config.device
    )
    
    start_time = time.time()
    diffusion_model.train_model(train_dataset, epochs=config.diffusion_epochs)
    diffusion_time = time.time() - start_time
    
    # é²æ£’æ•°æ®å¢å¼º
    start_time = time.time()
    augmented_dataset = robust_augment_dataset(train_dataset, diffusion_model, config)
    augment_time = time.time() - start_time
    
    # äº¤å‰éªŒè¯è®­ç»ƒ
    start_time = time.time()
    models, cv_results = cross_validation_training(augmented_dataset, train_dataset, config, device=config.device)
    gnn_time = time.time() - start_time
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    if models and cv_results:
        best_idx = np.argmax([r['f1'] for r in cv_results])
        best_model = models[best_idx]
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        model_path = os.path.join(output_path, "robust_gnn_model.pt")
        torch.save(best_model.state_dict(), model_path)
        
        # æµ‹è¯•æ¨¡å‹
        test_results = {}
        for test_file in test_files:
            test_name = os.path.splitext(os.path.basename(test_file))[0]
            test_dataset = load_dataset_quiet(test_file, config)
            
            if test_dataset:
                metrics = test_model_performance(best_model, test_dataset, config.device)
                test_results[test_name] = metrics
        
        # æ„å»ºç»“æœ
        result = {
            "model_name": f"{train_name}_robust_r{ratio_str}",
            "model_type": "Robust GNN with Quality-Controlled Augmentation",
            "training_info": {
                "original_positive": int(orig_pos),
                "original_negative": int(orig_neg),
                "original_ratio": float(orig_ratio),
                "augmented_positive": sum((d.y == 1).sum().item() for d in augmented_dataset),
                "augmented_negative": sum((d.y == 0).sum().item() for d in augmented_dataset),
                "target_ratio": config.target_ratio,
                "cv_avg_f1": float(np.mean([r['f1'] for r in cv_results])),
                "cv_avg_auc": float(np.mean([r.get('auc', 0) for r in cv_results])),
                "diffusion_time": diffusion_time,
                "augment_time": augment_time,
                "gnn_time": gnn_time,
                "total_time": diffusion_time + augment_time + gnn_time
            },
            "robust_config": {
                "quality_threshold": config.quality_threshold,
                "diversity_threshold": config.diversity_threshold,
                "domain_weight": config.domain_weight,
                "cv_folds": config.cv_folds,
                "max_augment_ratio": config.max_augment_ratio
            },
            "test_results": test_results
        }
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(output_path, "robust_results.json")
        with open(results_file, 'w') as f:
            json.dump(result, f, indent=2, default=str)
        
        return result
    
    return None


def quick_ratio_test():
    """å¿«é€Ÿæ¯”ä¾‹æµ‹è¯• - ä½¿ç”¨å•ä¸ªæ•°æ®é›†"""
    print(" å¿«é€Ÿæ¯”ä¾‹æµ‹è¯•")
    print("="*50)
    
    # é»˜è®¤æ–‡ä»¶è·¯å¾„
    train_file = '/mnt/data2/Yang/zhq-ds-main/Raw_data/DNA-573_Train.txt'
    test_files = [
        '/mnt/data2/Yang/zhq-ds-main/Raw_data/DNA-129_Test.txt',
        '/mnt/data2/Yang/zhq-ds-main/Raw_data/DNA-181_Test.txt', 
        '/mnt/data2/Yang/zhq-ds-main/Raw_data/DNA-46_Test.txt'
    ]
    
    # æµ‹è¯•ä¸åŒæ¯”ä¾‹
    ratios = [0.10, 0.15, 0.20, 0.25, 0.30]
    results = test_different_ratios(train_file, test_files, ratios)
    
    return results


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--ratio-test":
        # æ¯”ä¾‹æµ‹è¯•æ¨¡å¼
        quick_ratio_test()
    else:
        # æ­£å¸¸æ¨¡å¼
        main()